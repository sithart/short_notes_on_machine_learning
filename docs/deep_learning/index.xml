<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep_learning - Machine Learning</title>
    <link>https://chrisalbon.com/deep_learning/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Feb 2020 00:00:00 -0700</lastBuildDate>
    
        <atom:link href="https://chrisalbon.com/deep_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Check If PyTorch Is Using The GPU</title>
      <link>https://chrisalbon.com/deep_learning/pytorch/basics/check_if_pytorch_is_using_gpu/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/pytorch/basics/check_if_pytorch_is_using_gpu/</guid>
      <description>I find this is always the first thing I want to run when setting up a deep learning environment, whether a desktop machine or on AWS. These commands simply load PyTorch and check to make sure PyTorch can use the GPU.
Preliminaries # Import PyTorch import torch Check If There Are Multiple Devices (i.e. GPU cards) # How many GPUs are there? print(torch.cuda.device_count()) 1 Check Which Is The Current GPU? # Which GPU Is The Current GPU?</description>
    </item>
    
    <item>
      <title>Prevent Ubuntu 18.06 And Nvidia Drivers From Updating</title>
      <link>https://chrisalbon.com/deep_learning/setup/prevent_nvidia_drivers_from_upgrading/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/setup/prevent_nvidia_drivers_from_upgrading/</guid>
      <description>Ubuntu and NVIDIA&amp;rsquo;s drivers are continually being developed. Normally this is a good thing, however an update from either can break a working deep learning environment and by default these updates are automatic.
After successfully setting up your deep learning system and testing that it works, it is recommended to freeze the system in place by preventing automatic updates of both the NVIDIA drivers and Ubuntu.
Check NVIDIA Driver Version nvidia-smi</description>
    </item>
    
    <item>
      <title>Adding Dropout</title>
      <link>https://chrisalbon.com/deep_learning/keras/adding_dropout/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/adding_dropout/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers # Set random seed np.random.seed(0) Using TensorFlow backend.  Load IMDB Movie Review Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Network</title>
      <link>https://chrisalbon.com/deep_learning/keras/convolutional_neural_network/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/convolutional_neural_network/</guid>
      <description>Preliminaries import numpy as np from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers.convolutional import Conv2D, MaxPooling2D from keras.utils import np_utils from keras import backend as K # Set that the color channel value will be first K.set_image_data_format(&amp;#39;channels_first&amp;#39;) # Set seed np.random.seed(0) Using TensorFlow backend.  Load MNIST Image Data # Set image information channels = 1 height = 28 width = 28 # Load data and target from MNIST data (train_data, train_target), (test_data, test_target) = mnist.</description>
    </item>
    
    <item>
      <title>Feedforward Neural Network For Binary Classification</title>
      <link>https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_binary_classification/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_binary_classification/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers # Set random seed np.random.seed(0) Using TensorFlow backend.  Load Movie Review Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Feedforward Neural Network For Multiclass Classification</title>
      <link>https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_multiclass_classification/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_multiclass_classification/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import reuters from keras.utils.np_utils import to_categorical from keras.preprocessing.text import Tokenizer from keras import models from keras import layers # Set random seed np.random.seed(0) Using TensorFlow backend.  Load Movie Review Data # Set the number of features we want number_of_features = 5000 # Load feature and target data (train_data, train_target_vector), (test_data, test_target_vector) = reuters.load_data(num_words=number_of_features) # Convert feature data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Feedforward Neural Networks For Regression</title>
      <link>https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/feedforward_neural_network_for_regression/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn import preprocessing # Set random seed np.random.seed(0) Using TensorFlow backend.  Generate Training Data # Generate features matrix and target vector features, target = make_regression(n_samples = 10000, n_features = 3, n_informative = 3, n_targets = 1, noise = 0.0, random_state = 0) # Divide our data into training and test sets train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.</description>
    </item>
    
    <item>
      <title>k-Fold Cross-Validating Neural Networks</title>
      <link>https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/</guid>
      <description>If we have smaller data it can be useful to benefit from k-fold cross-validation to maximize our ability to evaluate the neural network&amp;rsquo;s performance. This is possible in Keras because we can &amp;ldquo;wrap&amp;rdquo; any neural network such that it can use the evaluation features available in scikit-learn, including k-fold cross-validation. To accomplish this, we first have to create a function that returns a compiled neural network. Next we use KerasClassifier (if we have a classifier, if we have a regressor we can use KerasRegressor) to wrap the model so it can be used by scikit-learn.</description>
    </item>
    
    <item>
      <title>LSTM Recurrent Neural Network</title>
      <link>https://chrisalbon.com/deep_learning/keras/lstm_recurrent_neural_network/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/lstm_recurrent_neural_network/</guid>
      <description>Oftentimes we have text data that we want to classify. While it is possible to use a type of convolutional network, we are going to focus on a more popular option: the recurrent neural network. The key feature of recurrent neural networks is that information loops back in the network. This gives recurrent neural networks a type of memory it can use to better understand sequential data. A popular choice type of recurrent neural network is the long short-term memory (LSTM) network which allows for information to loop backwards in the network.</description>
    </item>
    
    <item>
      <title>Neural Network Early Stopping</title>
      <link>https://chrisalbon.com/deep_learning/keras/neural_network_early_stopping/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/neural_network_early_stopping/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from keras.callbacks import EarlyStopping, ModelCheckpoint # Set random seed np.random.seed(0) Using TensorFlow backend.  Load Movie Review Text Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Neural Network Weight Regularization</title>
      <link>https://chrisalbon.com/deep_learning/keras/neural_network_weight_regularization/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/neural_network_weight_regularization/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from keras import regularizers # Set random seed np.random.seed(0) Using TensorFlow backend.  Load Movie Review Text Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Preprocessing Data For Neural Networks</title>
      <link>https://chrisalbon.com/deep_learning/keras/preprocessing_data_for_neural_networks/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/preprocessing_data_for_neural_networks/</guid>
      <description>Typically, a neural network&amp;rsquo;s parameters are initialized (i.e. created) as small random numbers. Neural networks often behave poorly when the feature values much larger than parameter values. Furthermore, since an observation&amp;rsquo;s feature values will are combined as they pass through individual units, it is important that all features have the same scale.
For these reasons, it is best practice (although not always necessary, for example when we have all binary features) to standardize each feature such that the feature&amp;rsquo;s values have the mean of 0 and the standard deviation of 1.</description>
    </item>
    
    <item>
      <title>Save Model Training Progress</title>
      <link>https://chrisalbon.com/deep_learning/keras/save_model_training_progress/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/save_model_training_progress/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from keras.callbacks import ModelCheckpoint # Set random seed np.random.seed(0) Load IMDB Movie Review Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Tuning Neural Network Hyperparameters</title>
      <link>https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras import models from keras import layers from keras.wrappers.scikit_learn import KerasClassifier from sklearn.model_selection import GridSearchCV from sklearn.datasets import make_classification # Set random seed np.random.seed(0) Using TensorFlow backend.  Generate Target And Feature Data # Number of features number_of_features = 100 # Generate features matrix and target vector features, target = make_classification(n_samples = 10000, n_features = number_of_features, n_informative = 3, n_redundant = 0, n_classes = 2, weights = [.</description>
    </item>
    
    <item>
      <title>Visualize Loss History</title>
      <link>https://chrisalbon.com/deep_learning/keras/visualize_loss_history/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/visualize_loss_history/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers import matplotlib.pyplot as plt # Set random seed np.random.seed(0) Using TensorFlow backend.  Load Movie Review Data # Set the number of features we want number_of_features = 10000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
    <item>
      <title>Visualize Neural Network Architecutre</title>
      <link>https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/</guid>
      <description>Preliminaries # Load libraries from keras import models from keras import layers from IPython.display import SVG from keras.utils.vis_utils import model_to_dot from keras.utils import plot_model Using TensorFlow backend.  Construct Neural Network Architecture # Start neural network network = models.Sequential() # Add fully connected layer with a ReLU activation function network.add(layers.Dense(units=16, activation=&amp;#39;relu&amp;#39;, input_shape=(10,))) # Add fully connected layer with a ReLU activation function network.add(layers.Dense(units=16, activation=&amp;#39;relu&amp;#39;)) # Add fully connected layer with a sigmoid activation function network.</description>
    </item>
    
    <item>
      <title>Visualize Performance History</title>
      <link>https://chrisalbon.com/deep_learning/keras/visualize_performance_history/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://chrisalbon.com/deep_learning/keras/visualize_performance_history/</guid>
      <description>Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers import matplotlib.pyplot as plt # Set random seed np.random.seed(0) Using TensorFlow backend.  Load Movie Review Data # Set the number of features we want number_of_features = 10000 # Load data and target vector from movie review data (train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer(num_words=number_of_features) train_features = tokenizer.</description>
    </item>
    
  </channel>
</rss>